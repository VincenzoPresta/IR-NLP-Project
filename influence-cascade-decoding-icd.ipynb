{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":13086776,"sourceType":"datasetVersion","datasetId":8281076},{"sourceId":13100200,"sourceType":"datasetVersion","datasetId":8295629},{"sourceId":13120702,"sourceType":"datasetVersion","datasetId":8310041}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<div align=\"center\">\n\n# **Influence Cascade Decoding (ICD)**  \n\n### *Notebook finale: IR-NLP & Analisi di Social Network e Media*  \n\n---\n\n**Ivan Prisco**  \n**Vincenzo Presta**  \n**Matteo Greco**  \n\n---\n</div>\n","metadata":{}},{"cell_type":"markdown","source":"# 1. Setup\nQueste celle si occupano di tutta la configurazione iniziale necessaria.\n \n- Installare le librerie principali (`bitsandbytes`, `transformers`, `accelerate`).\n- Mostrare le informazioni sulla GPU disponibile con `nvidia-smi`.\n- Configurare la memoria CUDA per evitare errori di allocazione.\n- Recuperare in modo sicuro il **token Hugging Face** dai *Kaggle Secrets*.\n- Eseguire il login su Hugging Face e stampa le info dell’account.\n- Definire una funzione `load_model()` che:\n  - carica il **tokenizer**,\n  - carica il modello (in questo caso **Mistral-7B-Instruct**) in quantizzazione 4-bit (per risparmiare memoria GPU),\n  - abilita gli **hidden states** per eventuali analisi avanzate.","metadata":{}},{"cell_type":"code","source":"!pip install -qU transformers accelerate bitsandbytes \n!nvidia-smi ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-22T14:13:43.002603Z","iopub.execute_input":"2025-09-22T14:13:43.002873Z","iopub.status.idle":"2025-09-22T14:15:24.582521Z","shell.execute_reply.started":"2025-09-22T14:13:43.002849Z","shell.execute_reply":"2025-09-22T14:15:24.581514Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport networkx as nx\nimport matplotlib.pyplot as plt\nimport os\nimport math\nimport pickle\nfrom kaggle_secrets import UserSecretsClient   \nfrom huggingface_hub import login, whoami      \nfrom transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\nfrom sentence_transformers import SentenceTransformer, util\nimport numpy as np\nimport re\nimport unicodedata\nimport random\nfrom collections import defaultdict\nimport nltk\nfrom nltk.corpus import stopwords \nimport json\nimport pandas as pd\nfrom sentence_transformers import SentenceTransformer, util\nimport time\nfrom typing import Dict\nfrom transformers import LogitsProcessor, LogitsProcessorList\nfrom pathlib import Path","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-22T14:15:24.584514Z","iopub.execute_input":"2025-09-22T14:15:24.584737Z","iopub.status.idle":"2025-09-22T14:15:59.759890Z","shell.execute_reply.started":"2025-09-22T14:15:24.584716Z","shell.execute_reply":"2025-09-22T14:15:59.759047Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# config memoria GPU\nos.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n\n#--- Hugginface\nuser_secrets = UserSecretsClient()  \nhf_token = user_secrets.get_secret(\"HF_TOKEN\")\nos.environ[\"HF_TOKEN\"] = hf_token\nlogin(token=hf_token)\n\ninfo = whoami()\nprint(f\"Login effettuato come: {info['name']}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-22T14:15:59.760744Z","iopub.execute_input":"2025-09-22T14:15:59.761453Z","iopub.status.idle":"2025-09-22T14:16:00.202248Z","shell.execute_reply.started":"2025-09-22T14:15:59.761426Z","shell.execute_reply":"2025-09-22T14:16:00.201612Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#--- Modello\nmodel_id = \"mistralai/Mistral-7B-Instruct-v0.3\" \n\ndef load_model(model_id):\n    bnb_config = BitsAndBytesConfig(load_in_4bit=True)\n    tokenizer = AutoTokenizer.from_pretrained(model_id)\n\n    model = AutoModelForCausalLM.from_pretrained(\n        model_id,\n        device_map=\"auto\",               \n        quantization_config=bnb_config,  \n        dtype=\"auto\",                    \n        attn_implementation=\"sdpa\"      \n    )\n    return tokenizer, model\n    \ntokenizer, model = load_model(model_id)\nmodel.config.output_hidden_states = True  ","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-09-22T14:16:00.203423Z","iopub.execute_input":"2025-09-22T14:16:00.203643Z","iopub.status.idle":"2025-09-22T14:18:41.479890Z","shell.execute_reply.started":"2025-09-22T14:16:00.203619Z","shell.execute_reply":"2025-09-22T14:18:41.479353Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"---","metadata":{}},{"cell_type":"markdown","source":"## Baseline Decoding\n### Funzione di baseline per la generazione\n\nSi definisce una funzione che permette di generare testo in modo standard utilizzando le tecniche di decoding già integrate nel modello.  \nQuesta baseline sarà utile come punto di riferimento per confrontare le prestazioni e le differenze rispetto a nuove strategie di decoding che verranno introdotte in seguito.  \n","metadata":{}},{"cell_type":"code","source":"def baseline_decoding(model, tokenizer, input_text, max_new_tokens=200, temperature=0.7, top_k=50):\n    input_ids = tokenizer.encode(input_text, return_tensors=\"pt\").to(model.device)\n\n    # Usa il token di padding corretto, o l'eos_token se non presente \n    pad_token_id = tokenizer.pad_token_id if tokenizer.pad_token_id is not None else tokenizer.eos_token_id\n\n    with torch.no_grad():\n        output_ids = model.generate(\n            input_ids,\n            max_new_tokens=max_new_tokens,\n            do_sample=True,\n            temperature=temperature,\n            top_k=top_k,\n            num_return_sequences=1,\n            pad_token_id=pad_token_id,\n            no_repeat_ngram_size=2\n        )\n\n    output_text = tokenizer.decode(output_ids[0], skip_special_tokens=True, clean_up_tokenization_spaces=True)\n    return output_text","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-22T14:18:41.481073Z","iopub.execute_input":"2025-09-22T14:18:41.481340Z","iopub.status.idle":"2025-09-22T14:18:41.486550Z","shell.execute_reply.started":"2025-09-22T14:18:41.481320Z","shell.execute_reply":"2025-09-22T14:18:41.485752Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"---","metadata":{}},{"cell_type":"markdown","source":"# 2. Creazione della rete","metadata":{}},{"cell_type":"markdown","source":"## 2.1 Strategie di simulazione dei token\n\nQuesta sezione implementa diverse **strategie di campionamento** dei token a partire\nda una distribuzione di probabilità generata dal modello. L’obiettivo è simulare\ndiversi scenari di generazione, così da analizzare in seguito le transizioni tra token\ne costruire le reti di influenza.\n\n### 1. Nucleus Sampling (*Top-p*)\n- Ordina i token in base alla probabilità.\n- Calcola la somma cumulata delle probabilità fino a superare la soglia `p` (tipicamente 0.9).\n- Considera solo quell’insieme ristretto di token (“nucleus”) e ne sceglie uno in modo casuale.\n  \n### 2. Temperature Sampling\n- Applica un fattore di **temperatura** alla distribuzione:\n  - `T < 1`: distribuzione più “spinta” verso i token più probabili → testi più conservativi.\n  - `T > 1`: distribuzione più piatta → maggiore esplorazione e creatività.\n  - `T = 0`: degenerazione in greedy decoding (si sceglie sempre il token più probabile).\n- Utile per controllare direttamente il livello di variabilità nelle simulazioni.\n\n### 3. Top-k Sampling\n- Seleziona i `k` token con probabilità più alta.\n- Campiona casualmente un token solo da questo sottoinsieme.\n---\n\n### Funzione `run_all_simulations`\nQuesta funzione orchestra l’intero processo di simulazione:\n\n- Riceve in input un **prompt iniziale** e parametri di configurazione (`num_simulazioni`, `max_steps`).  \n- Per ciascuna simulazione:\n  1. Inizializza la sequenza con il token finale del prompt.\n  2. Sceglie in modo casuale una **strategia di campionamento** tra *top-k*, *top-p* e *temperature* (con temperature diverse prese a caso).\n  3. Itera per un numero massimo di passi (`max_steps`), generando un token per volta.  \n  4. Salva per ogni passo una tupla `(token_id, probabilità)`.  \n\n- Restituisce una **matrice di simulazioni**, organizzata come una lista di liste:  \n  - Ogni **riga** rappresenta una simulazione completa (dall’inizio alla fine).  \n  - All’interno di ogni riga, i **passi della simulazione** sono salvati in ordine sequenziale.  \n  - Ogni passo è rappresentato da una **tupla** `(token_id, probabilità)`:\n    - `token_id`: identificativo numerico del token scelto dal vocabolario del modello,\n    - `probabilità`: valore della distribuzione al momento dell’estrazione di quel token.  \n\n---\n\nQui viene posta la base sperimentale per trasformare la generazione del modello in un problema di **analisi di rete**, utile per studiare il ruolo dei token e la propagazione dell’influenza.\n","metadata":{}},{"cell_type":"code","source":"prompt = \"The future of AI is\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-22T14:18:41.487273Z","iopub.execute_input":"2025-09-22T14:18:41.487520Z","iopub.status.idle":"2025-09-22T14:18:41.508517Z","shell.execute_reply.started":"2025-09-22T14:18:41.487499Z","shell.execute_reply":"2025-09-22T14:18:41.507842Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#i metodi prendono la distribuzione dei token e poi successivamente scelgono la strategia\n\ndef nucleus_sampling(probs, top_p=0.9): #1 STRATEGIA DI SIMULAZIONE\n    sorted_indices = np.argsort(probs)[::-1]\n    sorted_probs = probs[sorted_indices]\n    cumulative_probs = np.cumsum(sorted_probs)\n    cutoff_index = np.argmax(cumulative_probs > top_p) + 1\n    #trova il primo indice dove la somma cumulata supera p\n    top_p_indices = sorted_indices[:cutoff_index] #prendismo solo gli indici che ricadono nell'insieme\n    top_p_probs = sorted_probs[:cutoff_index]\n    top_p_probs = top_p_probs / top_p_probs.sum()\n \n    return np.random.choice(top_p_indices, p=top_p_probs)# estraiamo da quell'inisieme di indici un token casualmente\n \n \ndef temperature_sampling(probs, temperature=0.7): #2 STRATEGIA DI SIMULAZIONE\n    if temperature == 0: #se temperatura=0--> greedy decoding\n        return np.argmax(probs)\n    probs = np.log(probs + 1e-12) / temperature #le proababilità variano in base alla temperatura, T<1 \"raffredda\" , T>1 appiattisce aumentando la diversità\n    probs = np.exp(probs)#riportiamo in probabilita1 normali dopo che erano log probability\n    probs = probs / probs.sum() #normalizziamo\n    return np.random.choice(len(probs), p=probs)\n \n        \ndef run_all_simulations(model, tokenizer, num_simulazioni, max_steps, prompt):\n    all_tokens = []   # matrice simulazioni x step con (token, prob), ogni riga ha le info di ciascuna simulazione , in cui si memorizzano token e prob di token estratto\n    context_tokens = tokenizer(prompt, return_tensors=\"pt\")[\"input_ids\"].to(model.device)\n    device = context_tokens.device\n \n     #  prendi l'ultimo token del prompt(lo mettiamo perchè altrimenti nella pesatura degli archi non riusciremmo ad ottenere quello che vogliamo)\n    start_token = context_tokens[0, -1].item()\n \n    for j in range(num_simulazioni):\n        generated = context_tokens.clone()\n        tokens_sim = []   # (token_id, prob)\n \n        \n        #  inserisci subito il token finale del prompt con prob=0.0\n        tokens_sim.append((start_token, 0.0))\n        # scegli una strategia fissa per la simulazione\n        strategy = random.choice([\"top_k\", \"top_p\", \"temp\"])\n        if strategy == \"temp\":\n            temp = random.choice([0, 0.7, 1.0, 1.2, 1.5])  # esempio\n        print(f\"Simulazione {j} - Strategia: {strategy}\")\n \n        for _ in range(max_steps):\n            with torch.no_grad():\n                outputs = model(generated)\n                logits = outputs.logits[:, -1, :]\n                probs = torch.softmax(logits, dim=-1)[0]\n                probs_np = probs.detach().cpu().to(torch.float32).numpy() #sposta da gpu a cpu per essere analizzato\n \n \n            if strategy == \"top_k\":\n                top_k = 50\n                top_k_values, top_k_indices = torch.topk(probs, top_k)\n                top_k_probs = top_k_values / top_k_values.sum()\n                chosen_index = torch.multinomial(top_k_probs, 1) #campiona dai top k un token casualmente\n                next_token_id = top_k_indices[chosen_index].item() #prende l'id del token estratto\n            elif strategy == \"top_p\":\n                next_token_id = nucleus_sampling(probs_np, top_p=0.9)\n            else:  # temp\n                next_token_id = temperature_sampling(probs_np, temperature=temp)\n \n            tokens_sim.append((next_token_id, probs[next_token_id].item()))#qui generiamo il vettore in cui ciascun elemento è una tupla: (token scelto,probabilità)\n \n            next_token = torch.tensor([[next_token_id]], device=device)\n            generated = torch.cat((generated, next_token), dim=-1)\n        #print(\"tokens_sim: \",tokens_sim)\n \n        all_tokens.append(tokens_sim)\n\n \n        print(\" RISULTATO SIMULAZIONE\", j)\n        print(\"Token-level:\", tokens_sim)\n        #print(\"Word-level:\", word_probs)\n \n    return all_tokens","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"La cella successiva si occupa del salvataggio della matrice *all_tokens* derivante dalle simulazioni. ","metadata":{}},{"cell_type":"code","source":"# parametri \ndomain = \"AI\" #questo parametro serve a scopo esplicativo, da cambiare rispetto al dominio\nnum_simulazioni = 150 \nmax_steps = 200\n\n# Lancio simulazioni\nall_tokens= run_all_simulations(\n    model, tokenizer,\n    num_simulazioni=num_simulazioni,\n    max_steps=max_steps,\n    prompt=prompt\n)\n\nfile_name = f\"all_tokens_{num_simulazioni}sim_{max_steps}steps_{domain}.pkl\"\n\nwith open(file_name, \"wb\") as f:\n    pickle.dump(all_tokens, f)\n\nprint(f\"Simulazioni completate e matrice esportata in: {file_name}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"---\n### Caricamento della matrice\nQuesta cella serve a ricaricare da file la matrice delle simulazioni già esportata, evitando di dover ripetere il processo di generazione (costoso in termini di tempo e risorse).  \nIn questo modo si può subito disporre delle sequenze simulate e procedere ai passi successivi.\n","metadata":{}},{"cell_type":"code","source":"path = \"/kaggle/input/all-tokens-simulations/all_tokens_150sim_200steps_AI.pkl\"\n\n# Caricamento matrice con transizioni\nwith open(path, \"rb\") as f:\n    all_tokens = pickle.load(f)\n\nprint(\"Matrice ricaricata, numero simulazioni:\", len(all_tokens))\nprint(\"Esempio prima simulazione:\", all_tokens[0][:5])  # primi 5 token della prima simulazion","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-22T14:18:41.510372Z","iopub.execute_input":"2025-09-22T14:18:41.510569Z","iopub.status.idle":"2025-09-22T14:18:41.567903Z","shell.execute_reply.started":"2025-09-22T14:18:41.510554Z","shell.execute_reply":"2025-09-22T14:18:41.567195Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"---","metadata":{}},{"cell_type":"markdown","source":"## 2.2 Costruzione della rete \n\n","metadata":{}},{"cell_type":"markdown","source":"### Grafo dei token\n\nIn questa cella viene definita la funzione `build_token_graph`, che trasforma le sequenze di token generate nelle simulazioni in un **grafo orientato**:\n\n- I **nodi** rappresentano i token.  \n- Gli **archi** rappresentano le transizioni osservate tra token consecutivi.  \n- Il **nodo iniziale** è l’ultimo token del prompt, da cui partono le simulazioni.  \n\nIl grafo viene quindi creato a partire da tutte le simulazioni:\n- Si scorre la matrice *all_tokens*:\n    - Per ogni tupla (*token_id*, *prob*) si crea un nodo *token_id* nella rete.\n    - Per ogni coppia di tuple (*token_id_1*, *prob_1*), (*token_id_2*, *prob_2*) in una riga, si crea un arco orientato da *token_id_1* a *token_id_2* nella rete. \n","metadata":{}},{"cell_type":"code","source":"def build_token_graph(all_tokens, prompt_tokens):\n    \n    G = nx.DiGraph()\n\n    # nodo iniziale = ultimo token del prompt\n    start_token = prompt_tokens[0, -1].item()\n\n    G.add_node(start_token)\n\n    for sim in all_tokens:  # ogni simulazione\n        if not sim:  # simulazione vuota → salta\n            continue\n\n        # ora scorre i token consecutivi nella simulazione\n        for i in range(len(sim) - 1):\n            t1, _ = sim[i]\n            t2, _ = sim[i + 1]\n\n            if t1 not in G:\n                G.add_node(t1)\n            if t2 not in G:\n                G.add_node(t2)\n\n            if not G.has_edge(t1, t2):\n                G.add_edge(t1, t2)\n\n    return G\n    \nprompt_tokens = tokenizer(prompt, return_tensors=\"pt\")[\"input_ids\"].to(model.device)\nG = build_token_graph(all_tokens, prompt_tokens)\nprint(\"Rete creata.\")\nprint(\"Numero nodi:\", G.number_of_nodes())\nprint(\"Numero archi:\", G.number_of_edges())\n\n#debug\nfor node in G.nodes():\n    vicini = list(G.successors(node))\n    #print(f\"{node} → {vicini}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-22T14:18:41.568763Z","iopub.execute_input":"2025-09-22T14:18:41.569048Z","iopub.status.idle":"2025-09-22T14:18:41.650570Z","shell.execute_reply.started":"2025-09-22T14:18:41.569023Z","shell.execute_reply":"2025-09-22T14:18:41.650024Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"In questa fase, il grafo costruito è puramente strutturale: ogni arco è presente\no assente in base alle transizioni osservate, senza alcuna informazione\nquantitativa. Nella fase successiva verranno introdotti i pesi\nassociati agli archi, al fine di riflettere la forza o la rilevanza delle transizioni. ","metadata":{}},{"cell_type":"markdown","source":"### Assegnamento dei pesi agli archi della rete\nPer rendere il grafo informativo e adatto a modellare processi di\ndiffusione, è stato introdotto uno **schema di pesatura** che combina due\ncomponenti distinte: la stima empirica delle transizioni e la correlazione\nstatistica tra token.\n\n1. **Stima condizionata $\\hat{p}(v|u)$**  \n   La prima componente è la **probabilità condizionata stimata** di osservare un token $v$ successivamente a un token $u$, definita come:\n\n   $$\n   \\hat{p}(v|u) = \\frac{\\text{freq}(u \\to v)}{\\sum_{v'} \\text{freq}(u \\to v')}\n   $$\n\n   dove $\\text{freq}(u \\to v)$ indica il numero di volte in cui la transizione $(u,v)$ è stata osservata nelle simulazioni. Questa misura cattura l’aspetto puramente empirico della frequenza delle transizioni.\n\n2. **PPMI (Positive Pointwise Mutual Information)**  \n   La seconda componente è la \\textbf{Positive Pointwise Mutual Information} (PPMI), che valuta la forza statistica dell’associazione tra due token al di là della loro frequenza marginale. È definita come:\n\n   $$\n   \\text{PPMI}(u, v) = \\max \\left( \\log \\frac{p(u, v)}{p(u)\\,p(v)}, \\; 0 \\right)\n   $$\n\ndove: \n- $p(u,v)$ è la probabilità congiunta della coppia $(u,v)$;\n- $p(u)$ e $p(v)$ sono le probabilità marginali dei due token.\n\nPer rendere i punteggi confrontabili con le probabilità condizionate, i valori di PPMI vengono normalizzati nell’intervallo $[0,1]$, dividendo per il massimo valore di PMI..\n\n---\n\n#### Formula finale del peso\nIl peso assegnato a ciascun arco è una combinazione convessa delle due componenti:\n\n$$\nw(u, v) = \\alpha \\cdot \\hat{p}(v|u) + (1 - \\alpha) \\cdot \\text{PPMI}_{\\text{norm}}(u, v)\n$$\n\ndove:  \n- $\\alpha \\in [0,1]$ regola il bilanciamento tra frequenza empirica e correlazione semantica.   \n\nIl valore finale del peso viene forzato nell’intervallo $[0, 1]$.\n\n---\n\nQuesto approccio presenta diversi vantaggi:\n\n- riduce il numero di archi con peso nullo, grazie alla combinazione di due misure diverse; \n- tiene conto sia della **frequenza empirica** delle transizioni osservate, sia della **forza statistica** della loro associazione;  \n- fornisce una base più solida per modellare i processi di diffusione dell’influenza nella rete.\n","metadata":{}},{"cell_type":"code","source":"from collections import defaultdict\n\ndef compute_edge_weights(G, all_tokens, alpha=0.6):\n\n    # Dizionari per contare frequenze\n    freq_uv   = defaultdict(int)    # frequenza delle transizioni u->v\n    out_total = defaultdict(int)    # numero totale di uscite da u\n    in_total  = defaultdict(int)    # numero totale di ingressi in v\n\n    # Si contano tutte le transizioni nei dati\n    for sim in all_tokens:  # sim = [(token_id, prob), ...]\n        for i in range(len(sim) - 1):\n            u = sim[i][0]\n            v = sim[i + 1][0]\n            freq_uv[(u, v)] += 1\n            out_total[u]   += 1\n            in_total[v]    += 1\n\n    total_bigrams = sum(freq_uv.values()) or 1 # Totale dei bigrammi osservati\n\n    # Probabilità marginali per u e v\n    p_u = {u: out_total[u] / total_bigrams for u in out_total}\n    p_v = {v: in_total[v]  / total_bigrams for v in in_total}\n\n    # Calcola PPMI per ogni arco osservato\n    eps = 1e-12\n    pmi_vals = {}\n    for (u, v), c in freq_uv.items():\n        p_uv = c / total_bigrams\n        pmi  = math.log((p_uv + eps) / ((p_u.get(u, 0) + eps) * (p_v.get(v, 0) + eps)))\n        pmi_vals[(u, v)] = max(pmi, 0.0)  # PPMI (solo valori positivi)\n\n    # Normalizza PPMI in [0,1]\n    max_ppmi = max(pmi_vals.values()) if pmi_vals else 1.0\n\n    def scale_ppmi(x):\n        return 0.0 if max_ppmi <= 0 else (x / max_ppmi)\n\n    # Funzione per la stima condizionata p_hat(v|u)\n    def p_cond(u, v):\n        denom = out_total.get(u, 0)\n        return freq_uv.get((u, v), 0) / denom if denom > 0 else 0.0\n\n    # Assegna a ogni arco i pesi\n    for u, v in G.edges():\n        cond = p_cond(u, v)\n        pp = scale_ppmi(pmi_vals.get((u, v), 0.0))\n        w  = alpha * cond + (1 - alpha) * pp\n        G[u][v]['p_cond']  = cond\n        G[u][v]['ppmi']   = pp\n        G[u][v]['weight'] = min(1.0, max(0.0, w))  # forzo tra 0 e 1\n\n    return G\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-22T14:18:41.651454Z","iopub.execute_input":"2025-09-22T14:18:41.651692Z","iopub.status.idle":"2025-09-22T14:18:41.659201Z","shell.execute_reply.started":"2025-09-22T14:18:41.651668Z","shell.execute_reply":"2025-09-22T14:18:41.658627Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"In questo contesto, si è scelto di fissare il valore $\\alpha = 0.4$ per la costruzione definitiva della rete. Tale valore rappresenta un compromesso equilibrato: da un lato evita una rete eccessivamente polarizzata, dominata da archi molto forti accanto a numerosi archi debolissimi; dall’altro non conduce a una struttura troppo sparsa o vuota.\nLa distribuzione risultante appare dunque sufficientemente centrale, mantenendo un buon numero di archi attivi e fornendo una base solida per la fase di diffusione.","metadata":{}},{"cell_type":"code","source":"G = compute_edge_weights(G, all_tokens, alpha=0.4)\n\ncount = 0\n# Visualizzazione dei pesi\nfor u, v, data in G.edges(data=True):\n    print(f\"{u} → {v} : p_cond={data['p_cond']:.3f}, ppmi={data['ppmi']:.3f}, weight={data['weight']:.3f}\")\n    count += 1 \n    if count >= 10: \n        break","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-22T14:18:41.659898Z","iopub.execute_input":"2025-09-22T14:18:41.660354Z","iopub.status.idle":"2025-09-22T14:18:41.763707Z","shell.execute_reply.started":"2025-09-22T14:18:41.660331Z","shell.execute_reply":"2025-09-22T14:18:41.762997Z"},"scrolled":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### PLOT: Distribuzione dei pesi","metadata":{}},{"cell_type":"code","source":"weights = [data[\"weight\"] for _, _, data in G.edges(data=True)]\n\n# plot istogramma\nplt.figure(figsize=(6,4))\nplt.hist(weights, bins=30, color=\"skyblue\", edgecolor=\"black\")\nplt.xlabel(\"Peso arco\")\nplt.ylabel(\"Frequenza\")\nplt.title(\"Distribuzione dei pesi degli archi\")\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-22T14:18:41.764477Z","iopub.execute_input":"2025-09-22T14:18:41.764910Z","iopub.status.idle":"2025-09-22T14:18:42.164890Z","shell.execute_reply.started":"2025-09-22T14:18:41.764886Z","shell.execute_reply":"2025-09-22T14:18:42.164165Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Come precedentemente accennato, per $\\alpha = 0.4$ la distribuzione si allarga e mostra una maggiore variabilità. L’effetto combinato di frequenza e PPMI genera una gamma di pesi più diversificata, con un picco intorno a 0.2","metadata":{}},{"cell_type":"markdown","source":"---","metadata":{}},{"cell_type":"markdown","source":"# 3. Diffusione nella rete","metadata":{}},{"cell_type":"markdown","source":"Dopo aver costruito il grafo e assegnato i pesi agli archi, la fase successiva consiste nell’analisi della propagazione dell’influenza tra i token. Per questo scopo è stato adottato il modello di diffusione **Independent Cascade (IC)**, particolarmente adatto a contesti in cui le attivazioni avvengono in modo probabilistico lungo gli archi pesati della rete.\n\nI **seed**, ovvero i nodi che risultano essere attivi all'inizio del processo di diffusione sono stati definiti come i token contenuti nel prompt di partenza *{\"The future of AI is\"}*. Essi rappresentano le sorgenti di influenza da cui prende avvio la diffusione nella rete.","metadata":{}},{"cell_type":"markdown","source":"##  3.1 Estrazione dei seed\n\nIn questa cella viene definita la funzione `get_prompt_seeds`, che ha lo scopo di estrarre i **nodi seed** a partire dal prompt:\n\n- Il prompt viene **tokenizzato** e ciascun token viene associato al relativo ID.  \n- Ogni token del prompt viene aggiunto al grafo `G` come nodo (se non già presente).  \n- Viene creata la lista `seed_nodes`, che rappresenta i nodi di partenza per la diffusione dell’influenza.  \n- Per chiarezza, vengono stampati a schermo gli ID dei token seed insieme alla loro decodifica testuale.  \n\nIn questo modo la diffusione nel grafo inizia direttamente dai token presenti nel prompt fornito.\n","metadata":{}},{"cell_type":"code","source":"def get_prompt_seeds(G, tokenizer, prompt):\n    # tokenizza il prompt\n    prompt_tokens = tokenizer(prompt, return_tensors=\"pt\", add_special_tokens=False)[\"input_ids\"][0].tolist()\n    \n    seed_nodes = []\n    for tid in prompt_tokens:\n        if tid not in G:\n            G.add_node(tid)  # aggiungi nodo mancante\n        seed_nodes.append(tid)\n\n    # stampa i seed trovati\n    print(\"Token seed dal prompt\")\n    for tid in seed_nodes:\n        print(f\"{tid}:'{tokenizer.decode([tid])}'\")\n\n\n    return seed_nodes, G","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-22T14:18:42.165668Z","iopub.execute_input":"2025-09-22T14:18:42.165915Z","iopub.status.idle":"2025-09-22T14:18:42.170320Z","shell.execute_reply.started":"2025-09-22T14:18:42.165893Z","shell.execute_reply":"2025-09-22T14:18:42.169729Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 3.2 Independent Cascade\n\nIl modello IC si basa su una dinamica stocastica di attivazione dei nodi e ha le seguenti caratteristiche:\n\n- **Seed iniziali**: si parte da un insieme di nodi già attivi (nel nostro caso, i token del prompt).  \n- **Propagazione**: ad ogni step temporale, ogni nodo che è diventato attivo al passo precedente prova ad attivare i suoi vicini.  \n- **Probabilità di attivazione**: il tentativo di attivazione su un arco `(u → v)` ha successo con probabilità pari al peso dell’arco. In questo caso, i pesi derivano dalle simulazioni di generazione (frequenza × probabilità media della transizione).  \n- **Una sola occasione**: ogni nodo attivo ha una sola opportunità di attivare ciascun vicino. Se fallisce, non potrà più ritentare in futuro.  \n- **Terminazione**: il processo continua finché non ci sono più nuovi nodi attivati.  \n\nPer stimare in modo stabile l’influenza di ciascun nodo, il processo viene eseguito **molte volte (Monte Carlo)** e si calcola, per ogni nodo, la frazione di simulazioni in cui risulta attivato.  \n\nInfatti, per ciascun nodo $v$ viene calcolata la *probabilità empirica di attivazione*, definita come:\n$$\np_{\\text{att}}(v) =\n\\frac{\\# \\,\\mathrm{volte\\ in\\ cui}\\ v\\ \\mathrm{è\\ attivo}}\n{\\mathrm{numero\\ totale\\ di\\ simulazioni}}\n$$\nIn questo modo si ottiene un punteggio di influenza globale `influence_score[node] ∈ [0,1]` che misura quanto il nodo è “raggiungibile” dai seed secondo la dinamica IC.","metadata":{}},{"cell_type":"code","source":"def independent_cascade(G, seeds, num_iter=1000):\n    \"\"\"\n    Simula Independent Cascade su grafo pesato.\n    Restituisce un dizionario {nodo: punteggio_influenza}\n    \"\"\"\n    influence_count = defaultdict(int)\n\n    for _ in range(num_iter):\n        active = set(seeds)        # nodi attivi all'inizio\n        newly_active = set(seeds)\n\n        while newly_active:\n            next_new = set()\n            for u in newly_active:\n                for v in G.successors(u):\n                    if v not in active:\n                        prob = G[u][v].get(\"weight\", 0.0)\n                        if random.random() < prob:  # attivazione con prob\n                            next_new.add(v)\n            newly_active = next_new\n            active.update(newly_active)\n\n        # aggiorna conteggio influenza\n        for node in active:\n            influence_count[node] += 1\n\n    # normalizza in [0,1]\n    influence_score = {node: influence_count[node] / num_iter for node in G.nodes()} #è un dizionario\n    return influence_score\n\n# seeds dal prompt\nseed_nodes, G = get_prompt_seeds(G, tokenizer, prompt)\n\n# simulazione IC\nscores = independent_cascade(G, seed_nodes, num_iter=1000)\n\n# classifica i nodi più influenzati\nranked = sorted(scores.items(), key=lambda x: x[1], reverse=True)\nprint(\"\\n--- Nodi più influenzati ---\")\nfor node, score in ranked[:100]:\n    print(f\"{node}:'{tokenizer.decode([node])}' → {score:.3f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-22T14:18:42.172197Z","iopub.execute_input":"2025-09-22T14:18:42.172389Z","iopub.status.idle":"2025-09-22T14:18:48.611498Z","shell.execute_reply.started":"2025-09-22T14:18:42.172374Z","shell.execute_reply":"2025-09-22T14:18:48.610861Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 3.3  Costruzione del logit bias dai punteggi IC\n\nCostruita la rete di influenza e definiti i pesi degli archi, il passo successivo consiste nell’integrare queste informazioni all’interno del processo di generazione del modello linguistico. L’idea è di condizionare i \\textit{logit}\nprodotti dal modello in fase di decoding, in modo che la distribuzione finale dei token candidati rifletta non solo la conoscenza statistica del modello ma anche la struttura del grafo e eventuali bias esterni.\n","metadata":{}},{"cell_type":"markdown","source":"---","metadata":{}},{"cell_type":"markdown","source":"In questa cella vengono definiti gli strumenti per convertire i **punteggi di influenza (IC scores)** in un **bias** da applicare al modello:\n\n- Vengono scaricate le **stopword inglesi** tramite `nltk` e definite le principali **punteggiature**.  \n- La funzione `fixScore`:\n  - riceve in input un dizionario di punteggi IC per token,  \n  - decodifica ciascun token per identificarne il contenuto testuale,  \n  - assegna un **bias pieno** ai token considerati concetti importanti,  \n  - assegna un **bias ridotto (×0.8)** a stopword e punteggiatura,  \n  - calcola infine un dizionario `{token_id: bias}`.\n\nIn questo modo si modulano le probabilità di generazione, riducendo l’impatto di stopword e segni di punteggiatura a favore di token più informativi. Si noti che, anche se non si fosse applicato questo leggero fix agli score, il risultato sarebbe stato molto simile in quanto la generazione non si basa esclusivamente sulla rete ma anche sui punteggi interni del modello.","metadata":{}},{"cell_type":"code","source":"nltk.download(\"stopwords\")\n\nstop_words = set(stopwords.words(\"english\")) \n# set di punteggiatura comuni\npunctuation_tokens = {\n\n    \",\", \".\", \";\", \":\", \"-\", \"–\", \"—\", \"(\", \")\", \"[\", \"]\", \"{\", \"}\",\n\n    \"!\", \"?\", \"¿\", \"¡\", \"'\", '\"', \"…\", \"...\", \"``\", \"''\",\n\n    \"/\", \"\\\\\", \"|\", \"_\", \"+\", \"*\", \"&\", \"%\", \"$\", \"#\", \"@\", \"^\",\n\n    \"<\", \">\", \"=\", \"~\", \"`\",\"’\"}\n \ndef fixScore(scores, tokenizer):\n    \"\"\"\n    Trasforma i punteggi IC (scores) in un dizionario {token_id: bias}\n    con pesatura differenziata:\n      - concetti: bias pieno\n      - stopword/punteggiatura: bias ridotto\n    \"\"\"\n    score_bias = {}\n    for token_id, score in scores.items():\n        token_txt = tokenizer.decode([token_id]).lower().strip()\n        if not token_txt:  # salta token vuoti\n            continue\n        # stopword o punteggiatura → bias ridotto\n        if token_txt in stop_words or token_txt in punctuation_tokens:\n            factor = 0.8\n        else:\n            factor = 1.0\n        # calcolo bias\n        bias = score * factor\n        score_bias[token_id] = bias\n    return score_bias","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-22T14:18:48.612178Z","iopub.execute_input":"2025-09-22T14:18:48.612365Z","iopub.status.idle":"2025-09-22T14:18:50.156937Z","shell.execute_reply.started":"2025-09-22T14:18:48.612350Z","shell.execute_reply":"2025-09-22T14:18:50.156267Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"score_bias = fixScore(scores, tokenizer)\n\n# ordina per bias decrescente\nranked_bias = sorted(score_bias.items(), key=lambda x: x[1], reverse=True)\n\nprint(\"\\n--- Top 50 logit bias ---\")\nfor tid, bias in ranked_bias[:50]:\n    txt = tokenizer.decode([tid])\n    print(f\"{tid}:'{txt}' → {bias:.3f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-22T14:18:50.157663Z","iopub.execute_input":"2025-09-22T14:18:50.157870Z","iopub.status.idle":"2025-09-22T14:18:50.200728Z","shell.execute_reply.started":"2025-09-22T14:18:50.157849Z","shell.execute_reply":"2025-09-22T14:18:50.200149Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"---","metadata":{}},{"cell_type":"markdown","source":"# 4. Generazione","metadata":{}},{"cell_type":"markdown","source":"Il processo di generazione può essere descritto nei seguenti passi:\n\n- **Codifica del prompt**: Il prompt viene tokenizzato e fornito al modello per avviare la generazione.\n\n- **Calcolo dei logit**: a ogni passo di decoding, il modello produce un vettore di logit, cioè valori non normalizzati che rappresentano la propensione del modello a generare ciascun token del vocabolario.\n\n- **Selezione dei candidati**: Per rendere il calcolo più efficiente e mirato, vengono considerati i **100 token con logit più alto** secondo il modello.\n\n- **Applicazione del boosting** \n    Per ciascun candidato $v$ vengono calcolati due contributi:\n    - Peso dal grafo} $w(u,v)$: se esiste un arco dal token corrente $u$ verso $v$, si utilizza il peso calcolato inf fase di costruzione della rete.\n    - Bias esterno $\\text{bias}(v)$: punteggio assegnato al token $v$ dal processo di diffusione, che misura la probabilità empirica di attivazione del token a partire dai seed del prompt. Questo valore permette di enfatizzare i token che risultano più centrali nella propagazione dell’influenza nella rete.\n        \nQuesti due contributi vengono combinati e scalati dal parametro $\\lambda$, ottenendo così la **formula finale**:\n$$\n    \\text{logit\\_final}(v) = \\text{logit}(v) + \\lambda \\cdot (w(u,v) + \\text{bias}(v)).\n$$\n\n- **Campionamento**: dopo la modifica dei logit, si applica una *softmax* per ottenere la nuova distribuzione di probabilità e si campiona il prossimo token tra i **50 candidati più probabili** (strategia top k).\n\n- **Iterazioni**: il processo si ripete per il numero di passi prestabilito, generando così una sequenza in cui ogni scelta è influenzata sia dal modello che dalla rete di influenza.","metadata":{}},{"cell_type":"code","source":"def generate_with_logit_boosting(model, tokenizer, prompt, G, score_bias, lambda_=2.0, steps=200):\n    generated = tokenizer(prompt, return_tensors=\"pt\")[\"input_ids\"].to(model.device)\n \n    for step in range(steps):\n        with torch.no_grad():\n            outputs = model(generated)\n            logits = outputs.logits[:, -1, :].squeeze()\n \n        # ultimo token della sequenza\n        current_token = generated[0, -1].item()\n \n        # prendi i top-100 candidati\n        top100 = torch.topk(logits, 100)\n        indices = top100.indices.tolist()\n        values = top100.values.tolist()\n \n        # stampa i top-20 PRIMA\n        print(f\"\\n=== Step {step+1} ===\")\n        print(\"--- Top 20 PRIMA boosting ---\")\n        for idx, val in zip(top100.indices[:20].tolist(), top100.values[:20].tolist()):\n            print(f\"{idx}:'{tokenizer.decode([idx])}' → logit={val:.3f}\")\n        \n        # applica boosting solo sui top-100\n        boosted_logits = logits.clone()\n        for idx, val in zip(indices, values):\n            infl = score_bias.get(idx, 0.0)\n            w = 0.0\n            if current_token in G and idx in G[current_token]:\n                w = G[current_token][idx].get(\"weight\", 0.0)\n            boost = w + infl\n            boosted_logits[idx] = val + lambda_ * boost\n        \n        # softmax per probabilità DOPO boosting\n        boosted_probs = torch.softmax(boosted_logits, dim=-1)\n        \n        # stampa i top-20 DOPO\n        top20_after = torch.topk(boosted_logits, 20)\n        print(\"\\n--- Top 20 DOPO boosting ---\")\n        for idx, val in zip(top20_after.indices.tolist(), top20_after.values.tolist()):\n            print(f\"{idx}:'{tokenizer.decode([idx])}' → logit={val:.3f}\")\n\n        # prendi i top-50 dopo boosting\n        top50 = torch.topk(boosted_logits, 50)\n        final_indices = top50.indices\n        final_values = top50.values\n \n        # campiona il prossimo token dai top-50\n        probs = torch.softmax(final_values, dim=-1)\n        chosen_index = torch.multinomial(probs, 1).item()\n        next_token_id = final_indices[chosen_index].item()\n \n        # aggiungi il token generato\n        next_token = torch.tensor([[next_token_id]], device=generated.device)\n        generated = torch.cat((generated, next_token), dim=-1)\n \n        # stampa step\n        print(f\"\\nStep {step+1}\")\n        print(\"Scelto:\", next_token_id, \"→\", tokenizer.decode([next_token_id]))\n \n    return tokenizer.decode(generated[0], skip_special_tokens=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-22T14:31:24.216331Z","iopub.execute_input":"2025-09-22T14:31:24.216621Z","iopub.status.idle":"2025-09-22T14:31:24.226100Z","shell.execute_reply.started":"2025-09-22T14:31:24.216601Z","shell.execute_reply":"2025-09-22T14:31:24.225345Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"text = generate_with_logit_boosting(model, tokenizer, \"The future of AI is\", G, score_bias, lambda_= 1.5, steps=200)\nprint(\"\\n--- Testo finale ---\")\nprint(text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-22T14:31:28.381901Z","iopub.execute_input":"2025-09-22T14:31:28.382705Z","iopub.status.idle":"2025-09-22T14:34:39.320663Z","shell.execute_reply.started":"2025-09-22T14:31:28.382675Z","shell.execute_reply":"2025-09-22T14:34:39.319882Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 5. Analisi e test dei gradi di libertà","metadata":{}},{"cell_type":"markdown","source":"## 5.1 Analisi e valutazione del parametro $\\alpha$\nLa funzione `analyze_and_save_directed_basic` calcola e salva in un file JSON una serie di metriche strutturali e dinamiche di un grafo orientato pesato `G`. L’analisi viene condotta **in funzione del parametro $\\alpha$** utilizzato nello\n**schema di pesatura degli archi**, così da confrontare come cambia la struttura della rete e il processo di diffusione.\n\n\n####  Metriche calcolate\n\n1. Statistiche base\n    - **`num_nodes`**: numero di nodi $|V|$.  \n    - **`num_edges`**: numero di archi $|E|$.  \n    - **`density_directed`**: densità del grafo:\n  $$\n  \\text{density} = \\frac{|E|}{|V| \\cdot (|V|-1)}\n  $$\n\n2. Strength dei nodi\nSomma pesata degli archi entranti/uscenti:\n- $s^{in}(u) = \\sum_{(v,u)\\in E} w(v,u)$  \n- $s^{out}(u) = \\sum_{(u,v)\\in E} w(u,v)$  \n\nSi calcolano:\n- **`avg_in_strength`** = media di $s^{in}(u)$  \n- **`avg_out_strength`** = media di $s^{out}(u)$  \n\n3. Statistiche sui pesi\nAnalisi della distribuzione dei pesi $w(u,v)$:\n- $\\overline{w}$ (media), Var$(w)$ (varianza), max/min, mediana, quartili $Q_1,Q_3$  \n\n4. Connettività\n    - **`num_weakly_connected_components`**: numero di componenti se si ignora l’orientamento.  \n    - **`num_strongly_connected_components`**: componenti fortemente connesse.  \n\n5. Diffusione con Independent Cascade\nSi esegue il modello **Independent Cascade (IC)** con $200$ run, partendo dai token del prompt come seed.  \nPer ciascun $\\alpha$ si calcolano:\n    - **Coverage**:\n      $$\n      \\text{Cov} = \\frac{1}{|V|} \\sum_{v\\in V} p_{\\text{att}}(v)\n      $$\n      cioè la frazione media di nodi attivati.  \n\n    - **Entropy**:\n      $$\n       \\text{H} = - \\sum_{v \\in V} p_{\\text{att}}(v) \\log p_{\\text{att}}(v)\n      $$\n      misura della diversità: alta se l’attivazione è diffusa, bassa se è concentrata.\n  \n  \n---\nRipetendo queste analisi per più valori di $\\alpha$, si osserva come il bilanciamento tra **frequenza empirica** e **associazione semantica** modifica:\n- la struttura statica della rete (densità, pesi, strength, connettività),  \n- la dinamica di diffusione (coverage, entropia, nodi centrali).  \n\nQuesto permette di identificare i valori di $\\alpha$ che generano reti equilibrate ed efficaci per il boosting durante la generazione testuale.\n","metadata":{}},{"cell_type":"code","source":"def analyze_and_save_directed_basic(G, tokenizer, alpha=None, filename=\"network_metrics.json\", top_k=10):\n    metrics = {}\n    metrics[\"alpha\"] = alpha\n\n    # statistiche base\n    metrics[\"num_nodes\"] = int(G.number_of_nodes())\n    metrics[\"num_edges\"] = int(G.number_of_edges())\n    metrics[\"density_directed\"] = float(nx.density(G))\n\n    # strength (somma pesi entranti e uscenti)\n    in_strength = dict(G.in_degree(weight=\"weight\"))\n    out_strength = dict(G.out_degree(weight=\"weight\"))\n\n    metrics[\"avg_in_strength\"] = float(np.mean(list(in_strength.values())))\n    metrics[\"avg_out_strength\"] = float(np.mean(list(out_strength.values())))\n\n\n    # statistiche sui pesi\n    weights = [d[\"weight\"] for _, _, d in G.edges(data=True) if \"weight\" in d]\n    if weights:\n        metrics[\"avg_weight\"] = float(np.mean(weights))\n        metrics[\"var_weight\"] = float(np.var(weights))\n        metrics[\"max_weight\"] = float(np.max(weights))\n        metrics[\"min_weight\"] = float(np.min(weights))\n        metrics[\"median_weight\"] = float(np.median(weights))\n        metrics[\"q25_weight\"] = float(np.quantile(weights, 0.25))\n        metrics[\"q75_weight\"] = float(np.quantile(weights, 0.75))\n\n\n    # connettività\n    metrics[\"num_weakly_connected_components\"] = nx.number_weakly_connected_components(G)\n    metrics[\"num_strongly_connected_components\"] = nx.number_strongly_connected_components(G)\n\n    # lancia simulazioni \n    scores = independent_cascade(G, seeds, num_iter=200)\n\n    # copertura media (frazione nodi attivati)\n    coverage = sum(scores.values()) / len(scores)\n    results[\"coverage\"] = coverage\n\n    # entropia distribuzione influenza\n    probs = [s for s in scores.values() if s > 0]\n    entropy = -sum(p * math.log(p) for p in probs) if probs else 0.0\n    results[\"entropy\"] = entropy\n\n    # salvataggio\n    with open(filename, \"w\") as f:\n        json.dump(metrics, f, indent=2)\n\n    print(f\" Metriche di base orientate e pesate salvate in {filename}\")\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Risultati\n| α    | $\\overline{s^{in}}$ | $\\overline{s^{out}}$ | $\\overline{w}$ | Var$(w)$ | $w_{\\max}$ | $w_{\\min}$ | $w_{\\text{med}}$ | $[Q_1,Q_3]$       |\n|------|----------------------|----------------------|----------------|----------|------------|------------|------------------|-------------------|\n| 0    | 1.555                | 1.555                | 0.459          | 0.070    | 1.0        | 0.0000     | 0.399            | [0.257, 0.664]    |\n| 0.25 | 1.415                | 1.415                | 0.418          | 0.067    | 1.0        | 0.00018    | 0.356            | [0.221, 0.583]    |\n| 0.5  | 1.276                | 1.276                | 0.377          | 0.077    | 1.0        | 0.00035    | 0.286            | [0.164, 0.546]    |\n| 0.75 | 1.136                | 1.136                | 0.335          | 0.098    | 1.0        | 0.00053    | 0.202            | [0.094, 0.476]    |\n| 1    | 0.996                | 0.996                | 0.294          | 0.131    | 1.0        | 0.00071    | 0.111            | [0.014, 0.500]    |\n","metadata":{}},{"cell_type":"markdown","source":"---","metadata":{}},{"cell_type":"markdown","source":"## 5.2 Analisi e valutazione del parametro $\\lambda$ \n\nIn questo esperimento si analizza l’effetto del parametro **λ** sul modello influenzato dalla rete.\nL’obiettivo è studiare come λ modifichi la qualità e la stabilità dei testi generati, riducendo la variabilità tramite **più generazioni per ogni valore di λ**.\n\n\n#### Procedura\n1. **Prompt fisso**: tutte le generazioni partono dallo stesso prompt (`\"The future of AI is\"`).  \n2. **Valori di λ**: si testano diversi valori di intensità del boosting:  \n   $$\\lambda \\in \\{0.0, 1.0, 1.5, 2.0, 2.5, 5.0, 10.0, 100.0\\}$$  \n\n   - λ **basso (≈0–0.5)** → il modello resta simile al comportamento nativo.  \n   - λ **medio (1.5 - 2.5)** → la rete di influenza condiziona in modo marcato le scelte dei token.  \n   - λ **alto (>5)** → rischio di collasso: la generazione segue quasi esclusivamente la rete.  \n3. **Generazioni multiple**: per ogni λ si eseguono *num_runs* generazioni indipendenti (es. 30).  \n   I risultati di ogni run vengono salvati in file separati (`lambda_<valore>_results.json`) contenenti testo e metriche.  \n\n#### Metriche calcolate per ogni generazione\n- **Token influenti**:  \n  - conteggio assoluto $n_{\\text{influenti}}$ dei token presenti nella top-50 di `ranked_bias`,  \n  - frequenza normalizzata $f = \\tfrac{n_{\\text{influenti}}}{n_{\\text{token}}}$.  \n\n- **Perplexity (PPL)**: misura la plausibilità di una sequenza secondo una distribuzione di probabilità.  \n  Data una sequenza $x = (x_1, \\dots, x_N)$, la formula è:\n\n  $$\n  \\text{PPL}(x) = \\exp\\!\\Bigg(- \\frac{1}{N} \\sum_{t=1}^{N} \\log P(x_t \\mid x_{<t})\\Bigg)\n  $$\n\n  dove $P(x_t \\mid x_{<t})$ è la probabilità del token $x_t$ dato il contesto precedente.  \n\n  - **Caso λ = 0** → la sequenza è generata dal modello base; la perplexity si ottiene direttamente dalla loss nativa del modello (`compute_perplexity_native`).  \n  - **Caso λ > 0** → la sequenza è generata con boosting; la distribuzione viene modificata con i pesi della rete di influenza $G$ e i bias $\\text{prob\\_bias}$.  \n    In questo caso ricostruiamo esplicitamente le probabilità boostate e calcoliamo la perplexity coerente (`compute_perplexity_boosted`).  \n\n- **Similarità semantica**:  \n  Si utilizza SBERT (`all-MiniLM-L6-v2`) per calcolare gli embedding di prompt e testo, confrontandoli tramite similarità coseno:\n\n  $$\n  \\text{sim}(p, t) = \\frac{ \\langle e_p, e_t \\rangle }{ \\| e_p \\| \\cdot \\| e_t \\| }\n  $$\n\n","metadata":{}},{"cell_type":"code","source":"# parametri delle simulazioni\nprompt = \"The future of AI is\"\nlambda_values = [0.0, 1.0, 1.5, 2.0, 2.5, 5.0, 10.0, 100.0] \nsteps = 200\nnum_runs = 30    # numero di generazioni per ogni λ\n\n# modello SBERT\nsbert_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n\n# metriche\ndef compute_perplexity_native(text, model, tokenizer):\n    encodings = tokenizer(text, return_tensors=\"pt\").to(model.device)\n    with torch.no_grad():\n        outputs = model(**encodings, labels=encodings[\"input_ids\"])\n        loss = outputs.loss\n    return math.exp(loss.item())\n\ndef compute_perplexity_boosted(text, model, tokenizer, G, logit_bias, lambda_=2.0):\n    input_ids = tokenizer(text, return_tensors=\"pt\")[\"input_ids\"].to(model.device)\n    n_tokens = input_ids.size(1)\n    log_probs = []\n\n    for t in range(n_tokens - 1):\n        context = input_ids[:, :t+1]\n\n        with torch.no_grad():\n            outputs = model(context)\n            logits = outputs.logits[:, -1, :].squeeze()\n\n        current_token = context[0, -1].item()\n        next_token = input_ids[0, t+1].item()\n\n        boosted_logits = logits.clone()\n        topk = torch.topk(logits, 100)\n\n        for idx, val in zip(topk.indices.tolist(), topk.values.tolist()):\n            infl = logit_bias.get(idx, 0.0)\n            w = 0.0\n            if current_token in G and idx in G[current_token]:\n                w = G[current_token][idx].get(\"weight\", 0.0)\n            boost = w + infl\n            boosted_logits[idx] = val + lambda_ * boost\n\n        boosted_probs = torch.softmax(boosted_logits, dim=-1)\n        prob_next = boosted_probs[next_token].item()\n\n        log_probs.append(math.log(prob_next) if prob_next > 0 else float(\"-inf\"))\n\n    if len(log_probs) == 0 or float(\"-inf\") in log_probs:\n        return float(\"inf\")\n\n    avg_neg_log = -sum(log_probs) / len(log_probs)\n    return math.exp(avg_neg_log)\n\ninfluential_tokens = [tid for tid, bias in ranked_bias[:50]]\n\nfor lam in lambda_values:\n    print(f\"\\n=== Test multipli con λ={lam} ===\")\n    lambda_results = []\n\n    for run in range(num_runs):\n        print(f\"→ Run {run+1}/{num_runs}\")\n\n        text = generate_with_logit_boosting( \n            model, tokenizer, prompt,\n            G=G, logit_bias=logit_bias,\n            lambda_=lam, steps=steps  \n        )\n\n        # tokenizzazione e freq token influenti\n        ids = tokenizer(text, return_tensors=\"pt\")[\"input_ids\"].squeeze().tolist()\n        n_token = len(ids)\n        n_influenti = sum(1 for t in ids if t in influential_tokens)\n        freq_influenti = n_influenti / n_token if n_token > 0 else 0.0\n\n        # perplexity\n        if lam == 0.0:\n            ppl = compute_perplexity_native(text, model, tokenizer)\n        else:\n            ppl = compute_perplexity_boosted(text, model, tokenizer, G, logit_bias, lambda_=lam)\n\n        # similarity SBERT\n        embeddings = sbert_model.encode([prompt, text], convert_to_tensor=True)\n        sim = util.cos_sim(embeddings[0], embeddings[1]).item()\n\n        # formattazione\n        lambda_results.append({\n            \"run\": run,\n            \"prompt\": prompt,\n            \"lambda\": lam,\n            \"text\": text,\n            \"n_token\": n_token,\n            \"n_influenti\": n_influenti,\n            \"freq_influenti\": freq_influenti,\n            \"perplexity\": ppl,\n            \"sbert_similarity\": sim\n        })\n\n    # salvataggio\n    fname = f\"lambda_{lam}_results.json\"\n    with open(fname, \"w\", encoding=\"utf-8\") as f:\n        json.dump(lambda_results, f, ensure_ascii=False, indent=2)\n    print(f\"Risultati salvati in {fname}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Visualizzazione dei risultati: ","metadata":{}},{"cell_type":"code","source":"# Directory del dataset\ndata_dir = \"/kaggle/input/lambda-testing\"\n\n# Lista file\nfiles = [\n    \"lambda_0.0_results.json\",\n    \"lambda_1.0_results.json\",\n    \"lambda_1.5_results.json\",\n    \"lambda_2.0_results.json\",\n    \"lambda_2.5_results.json\",\n    \"lambda_5.0_results.json\",\n    \"lambda_10.0_results.json\",\n    \"lambda_100.0_results.json\"\n]\n\n# === Parsing risultati ===\nresults = []\nfor file in files:\n    path = os.path.join(data_dir, file)\n    with open(path, \"r\", encoding=\"utf-8\") as f:\n        data = json.load(f)\n\n    freq = [d[\"freq_influenti\"] for d in data]\n    ppl = [d[\"perplexity\"] for d in data]\n    sbert = [d[\"sbert_similarity\"] for d in data]\n\n    results.append({\n        \"file\": file,\n        \"lambda\": float(file.split(\"_\")[1].replace(\".json\", \"\")),\n        \"avg_freq_influenti\": sum(freq) / len(freq),\n        \"avg_perplexity\": sum(ppl) / len(ppl),\n        \"avg_sbert_similarity\": sum(sbert) / len(sbert)\n    })\n\ndf = pd.DataFrame(results).sort_values(by=\"lambda\")\ndisplay(df)\n\nbaseline = df[df[\"lambda\"] == 0.0].iloc[0]\n\n# Frequenza token influenti\nplt.figure(figsize=(7, 5))\nplt.plot(df[\"lambda\"], df[\"avg_freq_influenti\"], marker=\"o\", color=\"blue\")\nplt.axhline(y=baseline[\"avg_freq_influenti\"], color=\"red\", linestyle=\"--\", label=\"λ=0.0 baseline\")\nplt.xscale(\"log\")\nplt.xticks([1, 2, 5, 10, 100], labels=[1, 2, 5, 10, 100])\nplt.title(\"Avg Freq Influenti (log scale)\", fontsize=14)\nplt.xlabel(\"Lambda (log scale)\")\nplt.ylabel(\"Media\")\nplt.legend()\nplt.grid(True, linestyle=\"--\", alpha=0.6)\nplt.tight_layout()\nplt.savefig(\"freq_influenti_log.png\", dpi=300)  # <-- salva immagine\nplt.show()\n\n# Perplexity\nplt.figure(figsize=(7, 5))\nplt.plot(df[\"lambda\"], df[\"avg_perplexity\"], marker=\"o\", color=\"orange\")\nplt.axhline(y=baseline[\"avg_perplexity\"], color=\"red\", linestyle=\"--\", label=\"λ=0.0 baseline\")\nplt.xscale(\"log\")\nplt.xticks([1, 2, 5, 10, 100], labels=[1, 2, 5, 10, 100])\nplt.title(\"Avg Perplexity (log scale)\", fontsize=14)\nplt.xlabel(\"Lambda (log scale)\")\nplt.ylabel(\"Media\")\nplt.legend()\nplt.grid(True, linestyle=\"--\", alpha=0.6)\nplt.tight_layout()\nplt.savefig(\"perplexity_log.png\", dpi=300)  # <-- salva immagine\nplt.show()\n\n# SBERT-similarity\nplt.figure(figsize=(7, 5))\nplt.plot(df[\"lambda\"], df[\"avg_sbert_similarity\"], marker=\"o\", color=\"green\")\nplt.axhline(y=baseline[\"avg_sbert_similarity\"], color=\"red\", linestyle=\"--\", label=\"λ=0.0 baseline\")\nplt.xscale(\"log\")\nplt.xticks([1, 2, 5, 10, 100], labels=[1, 2, 5, 10, 100])\nplt.title(\"Avg SBERT Similarity (log scale)\", fontsize=14)\nplt.xlabel(\"Lambda (log scale)\")\nplt.ylabel(\"Media\")\nplt.legend()\nplt.grid(True, linestyle=\"--\", alpha=0.6)\nplt.tight_layout()\nplt.savefig(\"sbert_similarity_log.png\", dpi=300)  # <-- salva immagine\nplt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### Risultati\n| $\\lambda$ | Freq. token influenti | Perplexity | Similarità SBERT |\n|-----------|------------------------|------------|------------------|\n| 0.0       | 0.0858                 | 5.760      | 0.626            |\n| 1.0       | 0.1081                 | 4.467      | 0.641            |\n| 1.5       | 0.1163                 | 4.230      | 0.624            |\n| 2.0       | 0.1144                 | 4.369      | 0.628            |\n| 2.5       | 0.1217                 | 4.083      | 0.644            |\n| 5.0       | 0.1545                 | 3.087      | 0.646            |\n| 10.0      | 0.2156                 | 2.608      | 0.674            |\n| 100.0     | 0.3214                 | 1.230      | 0.598            |\n","metadata":{}},{"cell_type":"markdown","source":"---","metadata":{}},{"cell_type":"markdown","source":"# 6. Test aggiuntivi","metadata":{}},{"cell_type":"markdown","source":"## 6.1 A/B pair-wise test: LLM as a judge","metadata":{}},{"cell_type":"markdown","source":"### Introduzione\n\nPer valutare in modo oggettivo l’efficacia del meccanismo di **logit boosting**, \nè stato condotto un esperimento comparativo tra due varianti di generazione dello stesso modello:  \n- **baseline** → decoding standard,  \n- **boosted** → decoding con logit boosting guidato dalla rete di influenza ($\\lambda=1.5$).  \n\nSono stati generati **100 campioni indipendenti per ciascuna variante** (200 in totale), \nusando lo stesso prompt di partenza (*\"The future of AI is\"*).  \nLe coppie baseline/boosted sono state poi valutate da un **modello esterno in ruolo di giudice** \n(*Llama-3.1-8B-Instruct*), che assegna punteggi (scala 1–10) su tre dimensioni:  \n\n- *coherence* → fluidità e consistenza logica del testo,  \n- *informativeness* → quantità e rilevanza delle informazioni fornite,  \n- *factuality* → correttezza e plausibilità delle affermazioni.  \n\nOltre ai punteggi del giudice, sono state calcolate **metriche automatiche aggiuntive**:  \n- *token entropy* (diversità lessicale),  \n- *prompt coverage* (riuso del lessico del prompt),  \n- *semantic novelty* (distanza semantica tra baseline e boosted con SBERT).  \n\nIn questo modo il test combina una valutazione qualitativa (basata su un LLM \nusato come giudice) e quantitativa (metriche lessicali e semantiche), offrendo \nun quadro completo delle differenze introdotte dal boosting.\n\n---","metadata":{}},{"cell_type":"markdown","source":"### Step 1: Generazione e salvataggio dei testi\n\nQuesta cella definisce due funzioni per gestire la generazione e il salvataggio dei testi:\n\n- **`generate_variant`**: produce un output a partire da un prompt, scegliendo tra due varianti:\n    -  *baseline* (decoding standard),\n    -   *boosted* (decoding con logit boosting).\n\n\n- **`save_generations`**: esegue più generazioni per ciascun prompt, in entrambe le varianti, e salva i risultati in formato **JSONL** con metadati (prompt_id, variante, indice_generazione, testo).\n\nServe quindi a creare in modo sistematico il dataset di generazioni necessario per i test comparativi.\n","metadata":{}},{"cell_type":"code","source":"def generate_variant(model, tokenizer, prompt, variant, *,\n                     G=None, score_bias=None,\n                     max_new_tokens=200,    # baseline\n                     lambda_=1.5, steps=200,      # boosted\n                     seed=42):\n    if seed is not None:\n        import random, numpy as np, torch\n        random.seed(seed)\n        np.random.seed(seed)\n        torch.manual_seed(seed)\n        if torch.cuda.is_available():\n            torch.cuda.manual_seed_all(seed)\n\n    if variant == \"base\":\n        return baseline_decoding(\n            model, tokenizer, prompt,\n            max_new_tokens=max_new_tokens\n        )\n    elif variant == \"boosted\":\n        return generate_with_logit_boosting(\n            model, tokenizer, prompt, G, score_bias,\n            lambda_=lambda_, steps=steps\n        )\n    else:\n        raise ValueError(\"Variant must be 'base' or 'boosted'\")\n\n\ndef save_generations(prompts, model, tokenizer, G, logit_bias, n=5, path=\"outputs.jsonl\"):\n    total = len(prompts) * n * 2   # numero totale di generazioni\n    counter = 0\n\n    with open(path, \"w\", encoding=\"utf-8\") as f:\n        for pid, prompt in enumerate(prompts):\n            for i in range(n):\n                for variant in [\"base\", \"boosted\"]:\n                    counter += 1\n                    print(f\"[RUN] Generazione {counter}/{total} | prompt_id={pid}, sample={i}, variant={variant}\")\n\n                    text = generate_variant(\n                        model, tokenizer, prompt, variant,\n                        G=G, logit_bias=logit_bias, seed=i\n                    )\n                    rec = {\n                        \"prompt_id\": pid,\n                        \"variant\": variant,\n                        \"sample_id\": i,\n                        \"prompt\": prompt,\n                        \"text\": text\n                    }\n                    f.write(json.dumps(rec, ensure_ascii=False) + \"\\n\")\n\n    print(f\"[RUN] Completato! Generazioni salvate in: {path}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Run delle generazioni:","metadata":{}},{"cell_type":"code","source":"prompts = [\n    \"The future of AI is\"\n]\n\nG = G\nscore_bias = score_bias\n\nprint(\"[RUN] Generazione in corso...\")\nsave_generations(prompts, model, tokenizer, G, score_bias, n=50, path=\"outputs.jsonl\")\n\nprint(\"[RUN] Generazioni salvate in outputs.jsonl\")\nwith open(\"outputs_to_judge.jsonl\", \"r\", encoding=\"utf-8\") as f:\n    for i, line in enumerate(f):\n        if i >= 5:  # preview\n            break\n        print(line.strip())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"---","metadata":{}},{"cell_type":"markdown","source":"### Step 2: Setup del modello giudice e delle metriche aggiuntive\n\nQuesta cella si occupa del **setup del modello giudice e delle metriche aggiuntive**:\n\n* Definisce l’ID del modello usato come *judge* (`Llama-3.1-8B-Instruct`).\n* Configura la **quantizzazione 4-bit** tramite `BitsAndBytesConfig` per ridurre memoria e migliorare l’efficienza.\n* Carica tokenizer e modello da HuggingFace, con mappatura automatica su GPU e valutazione in sola lettura (`.eval()`).\n* Imposta il `pad_token_id` per garantire una gestione corretta delle sequenze.\n* Infine, carica **SBERT** (`all-MiniLM-L6-v2`), che verrà utilizzato per calcolare la *semantic novelty*.\n\nSi tratta dunque della cella di inizializzazione del modello giudice e dell’embedding model per le valutazioni.\n","metadata":{}},{"cell_type":"code","source":"JUDGE_MODEL_ID = \"meta-llama/Llama-3.1-8B-Instruct\"\n\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,   # usa 4-bit quantization\n    bnb_4bit_compute_dtype=\"bfloat16\",  # precisione dei calcoli\n    bnb_4bit_use_double_quant=True,\n)\n\nprint(\"[C1] Caricamento tokenizer...\")\ntokenizer = AutoTokenizer.from_pretrained(JUDGE_MODEL_ID)\n\nprint(\"[C1] Caricamento modello...\")\njudge_model = AutoModelForCausalLM.from_pretrained(\n    JUDGE_MODEL_ID,\n    device_map=\"auto\",\n    quantization_config=bnb_config,\n    trust_remote_code=True\n)\njudge_model.eval()\n\npad_token_id = tokenizer.pad_token_id or tokenizer.eos_token_id\n\n# === SBERT per novelty semantica ===\nsbert = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Questa cella definisce il **framework di valutazione automatica** delle coppie *baseline* vs *boosted* con il modello giudice:\n\n* **`SYSTEM_PROMPT`**: istruzioni dettagliate fornite al modello giudice, che descrivono come confrontare le due uscite e restituire solo un JSON con i punteggi su *coherence*, *informativeness* e *factuality*.\n* **`build_eval_prompt`**: costruisce il prompt di valutazione, includendo ID, testo di input e le due varianti da confrontare.\n* **`parse_strict_json`**: funzione robusta che taglia eventuale testo extra e tenta di decodificare il JSON prodotto dal modello.\n* **`judge_pair`**: funzione principale che\n\n  1. combina le istruzioni di sistema con i testi da valutare,\n  2. passa il prompt al modello giudice,\n  3. acquisisce l’output generato,\n  4. prova a fare il parsing in formato JSON,\n  5. restituisce i punteggi strutturati o, in caso di errore, il testo grezzo.\n\nQuesta parte implementa il meccanismo che permette al modello giudice di valutare in modo standardizzato le coppie di generazioni e di restituire i risultati in un formato facilmente analizzabile.\n","metadata":{}},{"cell_type":"code","source":"SYSTEM_PROMPT = \"\"\"You are an evaluator. \nYou MUST carefully compare the two outputs (\"base\" and \"boosted\") given the same prompt. \nFor each output, assign integer scores (1–10) on coherence, informativeness, and factuality. \nBase your evaluation ONLY on the given prompt and the two outputs. Ignore any external knowledge.\nReturn ONLY a valid JSON object with this schema:\n{\n \"prompt_id\": <int>,\n \"sample_id\": <int>,\n \"base\": {\"coherence\": <1-10>, \"informativeness\": <1-10>, \"factuality\": <1-10>},\n \"boosted\": {\"coherence\": <1-10>, \"informativeness\": <1-10>, \"factuality\": <1-10>}\n}\n\nScoring rules:\n- DO NOT give all 10s unless the text is truly flawless.\n- Use the full 1–10 scale realistically.\n- Different outputs should usually receive different scores.\n- Penalize vagueness, repetition, or lack of relevance.\n- Favor detailed, coherent, and factually plausible text.\n\nExample of good scoring:\n{\n \"prompt_id\": 99,\n \"sample_id\": 42,\n \"base\": {\"coherence\": 6, \"informativeness\": 5, \"factuality\": 7},\n \"boosted\": {\"coherence\": 8, \"informativeness\": 7, \"factuality\": 6}\n}\n\nNo other text, no explanations, no markdown.\n\"\"\"\n\n\ndef build_eval_prompt(prompt_id: int, sample_id: int, prompt: str, base_text: str, boosted_text: str) -> str:\n    return (\n        f\"prompt_id: {prompt_id}\\n\"\n        f\"sample_id: {sample_id}\\n\\n\"\n        f\"Prompt:\\n{prompt}\\n\\n\"\n        f\"Base output:\\n{base_text}\\n\\n\"\n        f\"Boosted output:\\n{boosted_text}\\n\\n\"\n        \"Now respond with ONLY the JSON object.\"\n    )\n\ndef parse_strict_json(text: str):\n    \"\"\"\n    Taglia il testo a [SYSTEM] (se presente) e prova a fare il parsing del JSON.\n    \"\"\"\n    if \"[SYSTEM]\" in text:\n        text = text.split(\"[SYSTEM]\")[0]  # tieni solo la parte prima\n    \n    if \"}\" in text:\n        candidate = text[:text.rfind(\"}\")+1]  # fino all'ultima graffa chiusa\n        try:\n            return json.loads(candidate)\n        except json.JSONDecodeError as e:\n            print(f\"[C1] JSONDecodeError: {e}\")\n            return None\n    return None\n\n\n\ndef judge_pair(prompt_id: int, sample_id: int, prompt: str, base_text: str, boosted_text: str, max_new_tokens=128) -> Dict:\n    user_prompt = build_eval_prompt(prompt_id, sample_id, prompt, base_text, boosted_text)\n    full_prompt = f\"[SYSTEM]\\n{SYSTEM_PROMPT}\\n\\n[USER]\\n{user_prompt}\\n\\n[ASSISTANT]\\n\"\n\n    '''print(\"====== PROMPT PASSATO AL MODELLO ======\")\n    print(full_prompt)\n    print(\"=======================================\")'''\n\n    inputs = tokenizer(full_prompt, return_tensors=\"pt\").to(judge_model.device)\n    input_len = inputs[\"input_ids\"].shape[1]\n\n    gen_out = judge_model.generate(\n        **inputs,\n        max_new_tokens=max_new_tokens,\n        do_sample=False,\n        pad_token_id=pad_token_id,\n        eos_token_id=tokenizer.eos_token_id,\n        return_dict_in_generate=True,\n    )\n\n    new_tokens = gen_out.sequences[0][input_len:]\n    raw_output = tokenizer.decode(new_tokens, skip_special_tokens=True).strip()\n    parsed = parse_strict_json(raw_output)\n\n    print(\"====== OUTPUT PARSED DEL MODELLO ======\")\n    print(parsed)\n    print(\"=======================================\")\n\n    if parsed and \"base\" in parsed and \"boosted\" in parsed:\n        return parsed\n    else:\n        return {\"error\": raw_output}\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Questa cella definisce funzioni di **pulizia del testo**:\n\n* `_clean_text` rimuove tag `<s>`, `</s>` e spazi extra.\n* `_tokens` tokenizza il testo in minuscolo, eliminando stopword e parole troppo corte.\n  Serve per preparare le generazioni a calcoli come *prompt coverage* o altre metriche lessicali.\n","metadata":{}},{"cell_type":"code","source":"# Utilità di pulizia\n\ndef _clean_text(t: str) -> str:\n    if not isinstance(t, str):\n        return \"\"\n    t = t.replace(\"<s>\", \" \").replace(\"</s>\", \" \")\n    return re.sub(r\"\\s+\", \" \", t).strip()\n\nSTOPWORDS = set(\"\"\"\na an and are as at be by for from has have i in is it its of on or that the to was were will with this these those into about over under between within without against across among\n\"\"\".split())\n\ndef _tokens(text: str):\n    toks = re.findall(r\"[a-zA-Z0-9']+\", text.lower())\n    return [t for t in toks if len(t) > 1 and t not in STOPWORDS]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Questa cella si occupa di definire le metriche aggiuntive calcolate:\n\n- **Token entropy** (diversità lessicale)  \n  Misura quanto è distribuita uniformemente la frequenza dei token:  \n  $$\n  H = - \\sum_i p_i \\log p_i\n  $$ \n  con $p_i$ probabilità empirica del token $i$.\n\n\n- **Semantic novelty** (distanza semantica tra baseline e boosted)  \n  Basata su embedding SBERT e similarità coseno:  \n  $$\n  1 - \\cos(\\mathbf{e}_{\\text{base}}, \\mathbf{e}_{\\text{boost}})\n  $$\n  Valori vicini a 0 indicano testi simili, valori alti segnalano maggiore novità.","metadata":{}},{"cell_type":"code","source":"# Metriche extra\ndef token_entropy(text: str) -> float:\n    toks = _tokens(text)\n    if not toks: return 0.0\n    _, counts = np.unique(toks, return_counts=True)\n    probs = counts / counts.sum()\n    return -(probs * np.log(probs)).sum()\n\ndef semantic_novelty(base_text: str, boosted_text: str) -> float:\n    if not base_text.strip() or not boosted_text.strip():\n        return 0.0\n    emb = sbert.encode([base_text, boosted_text], convert_to_tensor=True, normalize_embeddings=True)\n    sim = util.cos_sim(emb[0], emb[1]).item()\n    return 1.0 - sim","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Step 3: Giudizio\nQuesta cella implementa il **processo di valutazione e arricchimento dei risultati**:\n\n* Carica il file con le generazioni (`outputs_to_judge.jsonl`) e raggruppa le righe per coppia *(prompt\\_id, sample\\_id)*.\n* Per ciascuna coppia **baseline/boosted**:\n\n  * pulisce i testi,\n  * li sottopone al modello giudice con `judge_pair`,\n  * calcola le metriche extra (*token entropy, semantic novelty*).\n* Crea due nuove righe arricchite (una per baseline e una per boosted) che includono sia i punteggi del judge sia le metriche automatiche.\n* Salva tutto nel file di output `outputs_judged.jsonl`.\n* Infine stampa un riepilogo con il numero di coppie valutate, errori di parsing e righe non accoppiate.\n\nÈ la cella che produce i risutati finali per la fase di analisi comparativa\n","metadata":{}},{"cell_type":"code","source":"# path\nINPUT_JSONL  = \"/kaggle/input/llm-as-a-judge/outputs_to_judge.jsonl\"\nOUTPUT_JSONL = \"outputs_judged.jsonl\"\n\n# Caricamento input e accoppiamento base-boosted\nrows = [json.loads(line) for line in open(INPUT_JSONL, \"r\", encoding=\"utf-8\") if line.strip()]\nprint(f\"[C2] Righe lette: {len(rows)} dal file: {INPUT_JSONL}\")\n\nbuckets = defaultdict(list)\nfor r in rows:\n    key = (r[\"prompt_id\"], r[\"sample_id\"])\n    buckets[key].append(r)\n\naugmented_rows = []\nnum_pairs, num_parse_errors, leftovers = 0, 0, 0\n\nfor (pid, sid), items in buckets.items():\n    if len(items) < 2:\n        leftovers += len(items)\n        continue\n\n    base_row   = next((r for r in items if r[\"variant\"] == \"base\"), None)\n    boosted_row= next((r for r in items if r[\"variant\"] == \"boosted\"), None)\n    if not base_row or not boosted_row:\n        leftovers += len(items)\n        continue\n\n    base_text   = _clean_text(base_row[\"text\"])\n    boosted_text= _clean_text(boosted_row[\"text\"])\n    prompt      = base_row[\"prompt\"]\n\n    if num_pairs < 2:\n        print(f\"[C2] Pair (prompt_id={pid}, sample_id={sid})\")\n        print(\"Prompt:\", (prompt[:120] + \"...\") if len(prompt) > 120 else prompt)\n        print(\"Base:\", (base_text[:80] + \"...\") if len(base_text) > 80 else base_text)\n        print(\"Boosted:\", (boosted_text[:80] + \"...\") if len(boosted_text) > 80 else boosted_text)\n\n    try:\n        judge = judge_pair(pid, sid, prompt, base_text, boosted_text)\n    except Exception as e:\n        print(f\"[C2] ERRORE giudizio (pid={pid}, sid={sid}): {e}\")\n        num_parse_errors += 1\n        continue\n\n    # Metriche aggiornate\n    ent_base, ent_boost = token_entropy(base_text), token_entropy(boosted_text)\n    cov_base, cov_boost = prompt_coverage(base_text, prompt), prompt_coverage(boosted_text, prompt)\n    sem_novelty = semantic_novelty(base_text, boosted_text)\n\n    if not judge or \"base\" not in judge or \"boosted\" not in judge:\n        print(f\"[C2] Parsing fallito per (pid={pid}, sid={sid})\")\n        num_parse_errors += 1\n        continue\n\n    # riga base arricchita\n    row_base = dict(base_row,\n                    coherence=judge[\"base\"][\"coherence\"],\n                    informativeness=judge[\"base\"][\"informativeness\"],\n                    factuality=judge[\"base\"][\"factuality\"],\n                    entropy=ent_base,\n                    prompt_coverage=cov_base,\n                    semantic_novelty=sem_novelty)\n    augmented_rows.append(row_base)\n\n    # riga boosted arricchita\n    row_boost = dict(boosted_row,\n                     coherence=judge[\"boosted\"][\"coherence\"],\n                     informativeness=judge[\"boosted\"][\"informativeness\"],\n                     factuality=judge[\"boosted\"][\"factuality\"],\n                     entropy=ent_boost,\n                     prompt_coverage=cov_boost,\n                     semantic_novelty=sem_novelty)\n    augmented_rows.append(row_boost)\n\n    num_pairs += 1\n\n# salvataggio\nwith open(OUTPUT_JSONL, \"w\", encoding=\"utf-8\") as fout:\n    for r in augmented_rows:\n        fout.write(json.dumps(r, ensure_ascii=False) + \"\\n\")\n\nprint(\"-\" * 70)\nprint(f\"[C2] Coppie giudicate: {num_pairs} | Errori parsing: {num_parse_errors} | Non accoppiate: {leftovers}\")\nprint(f\"[C2] File con giudizi salvato in: {OUTPUT_JSONL}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Step 4: Analisi Comparativa\n\nQuesta cella carica i risultati giudicati, li raggruppa per coppia *baseline/boosted* e calcola le medie delle metriche principali. I valori vengono salvati in CSV e visualizzati tramite grafici a barre (sia multipli che singoli) per confrontare rapidamente le due varianti su coerenza, informatività, factualità ed extra metriche (*entropy, prompt coverage, semantic novelty*).\n","metadata":{}},{"cell_type":"code","source":"INPUT = Path(\"/kaggle/input/llm-as-a-judge/outputs_judged.jsonl\")\nOUTDIR = Path(\"./figures\")\nOUTDIR.mkdir(exist_ok=True)\n\n# caricamento json\nrows = [json.loads(l) for l in INPUT.read_text(encoding=\"utf-8\").splitlines() if l.strip()]\nprint(f\"[C3] Righe caricate: {len(rows)} da {INPUT}\")\n\ndef is_number(x):\n    try:\n        float(x); return True\n    except:\n        return False\n\n# raggruppamento (coppie)\ngroups = defaultdict(list)\nfor r in rows:\n    key = r.get(\"pair_id\") or r.get(\"prompt\") or r.get(\"prompt_text\") or r.get(\"input\") or \"NO_KEY\"\n    groups[key].append(r)\n\nclassic_metrics = [\"coherence\", \"informativeness\", \"factuality\"]\ncandidate_extra = [\"entropy\", \"semantic_novelty\"]\npresent_extra = [m for m in candidate_extra if any(m in r for r in rows)]\n\n# variant-wise\nvar_values = defaultdict(lambda: defaultdict(list))\n\nfor items in groups.values():\n    by_var = defaultdict(list)\n    for it in items:\n        if \"variant\" in it:\n            by_var[it[\"variant\"].lower()].append(it)\n\n    if \"base\" in by_var and \"boosted\" in by_var:\n        for b, s in zip(by_var[\"base\"], by_var[\"boosted\"]):\n            \n            # metriche classiche\n            for m in classic_metrics:\n                if m in b and m in s and is_number(b[m]) and is_number(s[m]):\n                    var_values[\"base\"][m].append(float(b[m]))\n                    var_values[\"boosted\"][m].append(float(s[m]))\n\n            # entropy \n            for m in [\"entropy\"]:\n                if m in present_extra:\n                    if is_number(b.get(m)) and is_number(s.get(m)):\n                        var_values[\"base\"][m].append(float(b[m]))\n                        var_values[\"boosted\"][m].append(float(s[m]))\n\n# semantic_novelty\nnovelty_vals = []\nfor r in rows:\n    if \"semantic_novelty\" in r and is_number(r[\"semantic_novelty\"]):\n        novelty_vals.append(float(r[\"semantic_novelty\"]))\n\n# tabella media semplice \nrecords = []\n\nfor m in classic_metrics + [\"entropy\", \"prompt_coverage\"]:\n    if m in present_extra or m in classic_metrics:\n        for var in [\"base\", \"boosted\"]:\n            vals = var_values[var].get(m, [])\n            if vals:\n                mean = np.mean(vals)\n                records.append({\"metric\": m, \"variant\": var, \"mean\": round(mean, 3)})\n\nif novelty_vals:\n    mean = np.mean(novelty_vals)\n    records.append({\"metric\": \"semantic_novelty\", \"variant\": \"boosted_only\", \"mean\": round(mean, 3)})\n\ndf = pd.DataFrame(records)\nprint(\"\\n=== Valori medi\")\nprint(df)\n\ndf.to_csv(OUTDIR / \"metrics_summary_simple.csv\", index=False)\n\n# plot\nmetrics_to_plot = classic_metrics + [m for m in [\"entropy\"] if m in present_extra]\ncolors = {\"base\": \"#007aff\", \"boosted\": \"#ff4d4d\"}\n\nncols = 2\nnrows = int(np.ceil(len(metrics_to_plot) / ncols))\n\nfig, axes = plt.subplots(nrows, ncols, figsize=(6*ncols, 5*nrows), sharey=False)\naxes = axes.flatten()\n\nfor ax, m in zip(axes, metrics_to_plot):\n        means, xs = [], []\n        for i, var in enumerate([\"base\", \"boosted\"]):\n            vals = var_values[var].get(m, [])\n            if vals:\n                xs.append(i)\n                means.append(np.mean(vals))\n\n        # plot nel grafico multiplo\n        ax.bar(xs, means, color=[colors[v] for v in [\"base\", \"boosted\"]])\n        ax.set_xticks(xs)\n        ax.set_xticklabels([\"Base\", \"Boosted\"])\n        ax.set_title(m)\n        if m in [\"coherence\", \"informativeness\", \"factuality\", \"entropy\"]:\n            ax.set_ylim(0, 10)\n        ax.grid(axis=\"y\", linestyle=\"--\", alpha=0.6)\n\n        for bar, val in zip(ax.patches, means):\n            ax.text(bar.get_x() + bar.get_width()/2, bar.get_height(),\n                    f\"{val:.2f}\", ha=\"center\", va=\"bottom\", fontsize=10, fontweight=\"bold\")\n\n        # plot e salvataggio singolo\n        fig_single, ax_single = plt.subplots(figsize=(5,5))\n        bars_single = ax_single.bar(xs, means, color=[colors[v] for v in [\"base\", \"boosted\"]])\n        ax_single.set_xticks(xs)\n        ax_single.set_xticklabels([\"Base\", \"Boosted\"])\n        ax_single.set_title(m)\n        ax_single.grid(axis=\"y\", linestyle=\"--\", alpha=0.6)\n\n        if m in [\"coherence\", \"informativeness\", \"factuality\", \"entropy\"]:\n            ax_single.set_ylim(1, 10)\n\n        for bar, val in zip(bars_single, means):\n            ax_single.text(bar.get_x() + bar.get_width()/2, bar.get_height(),\n                           f\"{val:.2f}\", ha=\"center\", va=\"bottom\", fontsize=10, fontweight=\"bold\")\n\n        fig_single.savefig(OUTDIR / f\"{m}.png\", dpi=150, bbox_inches=\"tight\")\n        plt.close(fig_single)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-22T13:45:00.730692Z","iopub.execute_input":"2025-09-22T13:45:00.731423Z","iopub.status.idle":"2025-09-22T13:45:01.710241Z","shell.execute_reply.started":"2025-09-22T13:45:00.731395Z","shell.execute_reply":"2025-09-22T13:45:01.709387Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"---","metadata":{}},{"cell_type":"markdown","source":"# 7. Ottimizzazioni","metadata":{}},{"cell_type":"markdown","source":"## 7.1 Logits Processor\n\nLa libreria `transformers` permette di personalizzare la fase di generazione\ntramite i **LogitsProcessor**, moduli che modificano i punteggi del modello\n(logits) prima della scelta del token successivo. Sono lo strumento pensato per\napplicare vincoli o bias senza dover riscrivere manualmente il ciclo di decoding.  \n\nIn questo lavoro è stata implementata una versione personalizzata,\n**GraphBiasLogitsProcessor**, che integra l’informazione della rete di influenza\ndirettamente nel processo di generazione. In questo modo il boosting viene\napplicato in maniera trasparente e nativa, con la possibilità di combinarlo\nfacilmente con altre tecniche di campionamento.  \n","metadata":{}},{"cell_type":"markdown","source":"Questa cella definisce l’integrazione del boosting tramite `LogitsProcessor`:\n\n* **`GraphBiasLogitsProcessor`**: sottoclasse di `LogitsProcessor` che modifica i logits del modello aggiungendo un bias proporzionale ai pesi della rete di influenza (`G`) e a un termine esterno (`logit_bias`), scalati da `λ`.\n* **`generate_with_boosting_fast`**: funzione di generazione che inserisce il processore personalizzato dentro `transformers.generate`, così il boosting viene applicato automaticamente durante la scelta dei token, combinabile con strategie di campionamento (`top-k`, ecc.).\n\nIn sintesi, è la versione “veloce e nativa” del logit boosting.\n","metadata":{}},{"cell_type":"code","source":"class GraphBiasLogitsProcessor(LogitsProcessor):\n    def __init__(self, G, score_bias, lambda_, tokenizer=None):\n        self.G = G\n        self.score_bias = score_bias\n        self.lambda_ = lambda_\n        self.tokenizer = tokenizer  # solo per le stampe leggibili\n\n    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor) -> torch.FloatTensor:\n        current_token = input_ids[0, -1].item()  # ultimo token generato\n        boosted_scores = scores.clone()\n\n        # seleziona solo i top-100 candidati\n        topk = torch.topk(scores, 100)\n        indices = topk.indices[0].tolist()\n        values = topk.values[0].tolist()\n\n        print(\"\\n=== Nuovo step ===\")\n        print(\"--- Top 20 PRIMA boosting ---\")\n        for idx, val in zip(topk.indices[0][:20].tolist(), topk.values[0][:20].tolist()):\n            token_str = self.tokenizer.decode([idx]) if self.tokenizer else str(idx)\n            print(f\"{idx}:'{token_str}' → logit={val:.3f}\")\n\n        # applica boosting solo sui top-100\n        for idx, val in zip(indices, values):\n            infl = self.score_bias.get(idx, 0.0)\n            w = 0.0\n            if current_token in self.G and idx in self.G[current_token]:\n                w = self.G[current_token][idx].get(\"weight\", 0.0)\n            boost = w + infl\n            if boost > 0:\n                boosted_scores[0, idx] = val + self.lambda_ * boost\n\n        top20_after = torch.topk(boosted_scores, 20)\n        print(\"\\n--- Top 20 DOPO boosting ---\")\n        for idx, val in zip(top20_after.indices[0].tolist(), top20_after.values[0].tolist()):\n            token_str = self.tokenizer.decode([idx]) if self.tokenizer else str(idx)\n            print(f\"{idx}:'{token_str}' → logit={val:.3f}\")\n\n        return boosted_scores\n\n\ndef generate_with_boosting_fast(model, tokenizer, prompt, G, score_bias, lambda_=2.0, max_new_tokens=50):\n    input_ids = tokenizer(prompt, return_tensors=\"pt\")[\"input_ids\"].to(model.device)\n    processor = GraphBiasLogitsProcessor(G, score_bias, lambda_, tokenizer)\n\n    output_ids = model.generate(\n        input_ids,\n        max_new_tokens=max_new_tokens,\n        do_sample=True, \n        top_k=50,\n        logits_processor=LogitsProcessorList([processor])\n    )\n\n    return tokenizer.decode(output_ids[0], skip_special_tokens=True, clean_up_tokenization_spaces=True)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-22T14:38:41.798957Z","iopub.execute_input":"2025-09-22T14:38:41.799558Z","iopub.status.idle":"2025-09-22T14:38:41.808942Z","shell.execute_reply.started":"2025-09-22T14:38:41.799533Z","shell.execute_reply":"2025-09-22T14:38:41.808329Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"generate_with_boosting_fast(model, tokenizer, prompt, G, score_bias, lambda_=1.5, max_new_tokens= 200) ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-22T14:38:53.975683Z","iopub.execute_input":"2025-09-22T14:38:53.975943Z","iopub.status.idle":"2025-09-22T14:39:12.056310Z","shell.execute_reply.started":"2025-09-22T14:38:53.975928Z","shell.execute_reply":"2025-09-22T14:39:12.055688Z"},"scrolled":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Per verificarne i benefici, è stato condotto un confronto con l’approccio manuale: a parità di condizioni ($\\lambda=1.5$, sequenza di ~200 token), il metodo tradizionale ha richiesto oltre 3 minuti, mentre con LogitsProcessor il tempo è stato ridotto a meno di 20 secondi, con un guadagno di efficienza pari a circa **88.9%**.\n\n| **Metodo**                        | **Tempo (mm:ss)** | **Speedup**   |\n|-----------------------------------|-------------------|---------------|\n| Manuale (ciclo python esplicito)  | 03:07.30          | 1.0×          |\n| Con LogitsProcessor (C++/Cuda)    | 00:18.97          | ~9.9×         |\n\nIl miglioramento è spiegabile dal fatto che la funzione `model.generate()` sfrutta codice ottimizzato in C++ e CUDA, molto più efficiente rispetto a un ciclo Python puro, consentendo uno speedup rilevante nei tempi di generazione.\n","metadata":{}}]}